---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ffp-cluster-eksctl
  region: ap-northeast-2
  version: "1.15"
vpc:
  cidr: 10.16.0.0/16                    # 10.16.0.0   ~ 10.16.255.255, 65536 (optional, must match CIDR used by the given VPC)
  clusterEndpoints:
    privateAccess: true
    publicAccess: true
nodeGroups:
  - name: ffp-unmanaged-ng-proxy
    tags:
      nodegroup-name: ffp-unmanaged-ng-proxy
    labels:
      role: proxy
      nodegroup-type: unmanaged
      ec2-type: t3.medium
    instanceType: t3.medium
    # https://eksctl.io/usage/autoscaling/
    availabilityZones: ["ap-northeast-2a", "ap-northeast-2b", "ap-northeast-2c"]
    minSize: 1
    maxSize: 6
    desiredCapacity: 1
    volumeSize: 20
    volumeType: gp2
    ami: ami-08a18de5609e8f781
    privateNetworking: true             # if only 'Private' subnets are given, this must be enabled
    ssh:
      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key
    iam:
      withAddonPolicies:
        ########################################################################
        # An example of ClusterConfig object with access to CSI drivers:
        ########################################################################
        ebs: true    # CSI Driver
        efs: true    # CSI Driver
        fsx: true    # CSI Driver
        ########################################################################
        # An example of ClusterConfig object compatible with the "app-dev" quickstart
        # profile. See also: https://github.com/weaveworks/eks-quickstart-app-dev
        ########################################################################
        albIngress: true
        autoScaler: true
        cloudWatch: true
        ########################################################################
        # https://eksctl.io/usage/iam-policies/
        ########################################################################
        imageBuilder: true
    # ==> ERROR occur :  # test for error - Error: timed out (after 25m0s) waiting for at least 1 nodes to join the cluster and become ready in "ffp-unmanaged-ng-proxy"
    # https://github.com/weaveworks/eksctl/issues/1482
    # ==> Finally i managed to fix the problem.
    # The issue was the unavailability of the worked nodes in the vpn to connect to ECR repo.
    # hence none of the pods were able to run that's why the nodes were not able to join the cluster.
    # In our case worker nodes were in private subnet hence they were not able to connect to ECR as we do not had Natgateway to conncet to ECR service.
    # I had created a VPC endpoint to conncet for ECR then my worker nodes were able to install the pods and then successfully able to join the cluster.
    #preBootstrapCommands:
    #  # allow docker registries to be deployed as cluster service ( Need. ECR )
    #  - 'echo {\"insecure-registries\": [\"172.20.0.0/16\",\"10.100.0.0/16\"]} > /etc/docker/daemon.json'
    #  - "systemctl restart docker"
    #  - 'echo {\"insecure-registries\": [\"644960261046.dkr.ecr.ap-northeast-2.amazonaws.com\"]} > /etc/docker/daemon.json'
#  - name: ffp-unmanaged-ng-worker
#    tags:
#      nodegroup-name: ffp-unmanaged-ng-worker
#    labels:
#      role: worker
#      nodegroup-type: unmanaged
#      ec2-type: t3.medium
#    instanceType: t3.medium
#    # https://eksctl.io/usage/autoscaling/
#    availabilityZones: ["ap-northeast-2a", "ap-northeast-2b", "ap-northeast-2c"]
#    minSize: 1
#    maxSize: 3
#    desiredCapacity: 1
#    volumeSize: 20
#    volumeType: gp2
#    ami: ami-08a18de5609e8f781
#    privateNetworking: true
#    ssh:
#      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key
#    iam:
#      # https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html
#      #attachPolicyARNs:
#      #  - arn:aws:iam::644960261046:role/EKS-IAM-ROLE
#      withAddonPolicies:
#        ########################################################################
#        # An example of ClusterConfig object with access to CSI drivers:
#        ########################################################################
#        ebs: true    # CSI Driver
#        efs: true    # CSI Driver
#        fsx: true    # CSI Driver
#        ########################################################################
#        # An example of ClusterConfig object compatible with the "app-dev" quickstart
#        # profile. See also: https://github.com/weaveworks/eks-quickstart-app-dev
#        ########################################################################
#        albIngress: true
#        autoScaler: true
#        cloudWatch: true
#        ########################################################################
#        # https://eksctl.io/usage/iam-policies/
#        ########################################################################
#        imageBuilder: true
    # ==> ERROR occur :  # test for error - Error: timed out (after 25m0s) waiting for at least 1 nodes to join the cluster and become ready in "ffp-unmanaged-ng-proxy"
    # https://github.com/weaveworks/eksctl/issues/1482
    # ==> Finally i managed to fix the problem.
    # The issue was the unavailability of the worked nodes in the vpn to connect to ECR repo.
    # hence none of the pods were able to run that's why the nodes were not able to join the cluster.
    # In our case worker nodes were in private subnet hence they were not able to connect to ECR as we do not had Natgateway to conncet to ECR service.
    # I had created a VPC endpoint to conncet for ECR then my worker nodes were able to install the pods and then successfully able to join the cluster.
    #preBootstrapCommands:
    #  # allow docker registries to be deployed as cluster service ( Need. ECR )
    #  - 'echo {\"insecure-registries\": [\"172.20.0.0/16\",\"10.100.0.0/16\"]} > /etc/docker/daemon.json'
    #  - "systemctl restart docker"
    #  - 'echo {\"insecure-registries\": [\"644960261046.dkr.ecr.ap-northeast-2.amazonaws.com\"]} > /etc/docker/daemon.json'
#cloudWatch:
#  clusterLogging:
#    # enable specific types of cluster control plane logs
#    enableTypes: ["audit", "authenticator", "controllerManager"]
#    # all supported types: "api", "audit", "authenticator", "controllerManager", "scheduler"
#    # supported special values: "*" and "all
---
