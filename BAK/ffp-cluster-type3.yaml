---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ffp-cluster-type7
  region: ap-northeast-2
  version: "1.15"
vpc:
  #id: vpc-011c2cc9193514f37             # (optional, must match VPC ID used for each subnet below)
  cidr: 10.16.0.0/16                    # 10.16.0.0   ~ 10.16.255.255, 65536 (optional, must match CIDR used by the given VPC)
  subnets:
    private: # AWS > VPC > List
      ap-northeast-2a: # Only 1 ID
        id: subnet-0f7101ad7cff51732
        #cidr: 10.16.32.128/26           # 10.16.32.128 ~ 10.16.32.191, 64(59) (optional, must match CIDR used by the given subnet)
      ap-northeast-2b: # Only 1 ID
        id: subnet-0206185592766d486
        #cidr: 10.16.64.128/26           # 10.16.64.128 ~ 10.16.64.191, 64(59) (optional, must match CIDR used by the given subnet)
      ap-northeast-2c: # Only 1 ID
        id: subnet-092fe63be54b653a4
        #cidr: 10.16.128.128/26           # 10.16.128.128 ~ 10.16.128.191, 64(59) (optional, must match CIDR used by the given subnet)
  clusterEndpoints:
    privateAccess: true
    publicAccess: true
  #nat:
  #  gateway: HighlyAvailable           # other options: Disable, Single (default)
nodeGroups:
  - name: ffp-unmanaged-ng-proxy
    tags:
      nodegroup-name: ffp-unmanaged-ng-proxy
    labels:
      role: proxy
      nodegroup-type: unmanaged
      ec2-type: t3.medium
    instanceType: t3.medium
    # https://eksctl.io/usage/autoscaling/
    availabilityZones: ["ap-northeast-2a", "ap-northeast-2b", "ap-northeast-2c"]
    minSize: 1
    maxSize: 3
    desiredCapacity: 1
    volumeSize: 20
    volumeType: gp2
    ami: ami-08a18de5609e8f781
    privateNetworking: true             # if only 'Private' subnets are given, this must be enabled
    ssh:
      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key
    iam:
      #attachPolicyARNs:
      #  - arn:aws:iam::644960261046:role/EKS-IAM-ROLE
      withAddonPolicies:
        ########################################################################
        # An example of ClusterConfig object with access to CSI drivers:
        ########################################################################
        ebs: true    # CSI Driver
        efs: true    # CSI Driver
        fsx: true    # CSI Driver
        ########################################################################
        # An example of ClusterConfig object compatible with the "app-dev" quickstart
        # profile. See also: https://github.com/weaveworks/eks-quickstart-app-dev
        ########################################################################
        albIngress: true
        autoScaler: true
        cloudWatch: true
        ########################################################################
        # https://eksctl.io/usage/iam-policies/
        ########################################################################
        imageBuilder: true
    # ==> ERROR occur :  # test for error - Error: timed out (after 25m0s) waiting for at least 1 nodes to join the cluster and become ready in "ffp-unmanaged-ng-proxy"
    # https://github.com/weaveworks/eksctl/issues/1482
    # ==> Finally i managed to fix the problem.
    # The issue was the unavailability of the worked nodes in the vpn to connect to ECR repo.
    # hence none of the pods were able to run that's why the nodes were not able to join the cluster.
    # In our case worker nodes were in private subnet hence they were not able to connect to ECR as we do not had Natgateway to conncet to ECR service.
    # I had created a VPC endpoint to conncet for ECR then my worker nodes were able to install the pods and then successfully able to join the cluster.
    #preBootstrapCommands:
    #  # allow docker registries to be deployed as cluster service ( Need. ECR )
    #  - 'echo {\"insecure-registries\": [\"172.20.0.0/16\",\"10.100.0.0/16\"]} > /etc/docker/daemon.json'
    #  - "systemctl restart docker"
    #  - 'echo {\"insecure-registries\": [\"644960261046.dkr.ecr.ap-northeast-2.amazonaws.com\"]} > /etc/docker/daemon.json'
  - name: ffp-unmanaged-ng-worker
    tags:
      nodegroup-name: ffp-unmanaged-ng-worker
    labels:
      role: worker
      nodegroup-type: unmanaged
      ec2-type: t3.medium
    instanceType: t3.medium
    # https://eksctl.io/usage/autoscaling/
    availabilityZones: ["ap-northeast-2a", "ap-northeast-2b", "ap-northeast-2c"]
    minSize: 1
    maxSize: 3
    desiredCapacity: 1
    volumeSize: 20
    volumeType: gp2
    ami: ami-08a18de5609e8f781
    privateNetworking: true
    ssh:
      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key
    iam:
      # https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html
      #attachPolicyARNs:
      #  - arn:aws:iam::644960261046:role/EKS-IAM-ROLE
      withAddonPolicies:
        ########################################################################
        # An example of ClusterConfig object with access to CSI drivers:
        ########################################################################
        ebs: true    # CSI Driver
        efs: true    # CSI Driver
        fsx: true    # CSI Driver
        ########################################################################
        # An example of ClusterConfig object compatible with the "app-dev" quickstart
        # profile. See also: https://github.com/weaveworks/eks-quickstart-app-dev
        ########################################################################
        albIngress: true
        autoScaler: true
        cloudWatch: true
        ########################################################################
        # https://eksctl.io/usage/iam-policies/
        ########################################################################
        imageBuilder: true
    # ==> ERROR occur :  # test for error - Error: timed out (after 25m0s) waiting for at least 1 nodes to join the cluster and become ready in "ffp-unmanaged-ng-proxy"
    # https://github.com/weaveworks/eksctl/issues/1482
    # ==> Finally i managed to fix the problem.
    # The issue was the unavailability of the worked nodes in the vpn to connect to ECR repo.
    # hence none of the pods were able to run that's why the nodes were not able to join the cluster.
    # In our case worker nodes were in private subnet hence they were not able to connect to ECR as we do not had Natgateway to conncet to ECR service.
    # I had created a VPC endpoint to conncet for ECR then my worker nodes were able to install the pods and then successfully able to join the cluster.
    #preBootstrapCommands:
    #  # allow docker registries to be deployed as cluster service ( Need. ECR )
    #  - 'echo {\"insecure-registries\": [\"172.20.0.0/16\",\"10.100.0.0/16\"]} > /etc/docker/daemon.json'
    #  - "systemctl restart docker"
    #  - 'echo {\"insecure-registries\": [\"644960261046.dkr.ecr.ap-northeast-2.amazonaws.com\"]} > /etc/docker/daemon.json'
#cloudWatch:
#  clusterLogging:
#    # enable specific types of cluster control plane logs
#    enableTypes: ["audit", "authenticator", "controllerManager"]
#    # all supported types: "api", "audit", "authenticator", "controllerManager", "scheduler"
#    # supported special values: "*" and "all
---
